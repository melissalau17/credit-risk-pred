# -*- coding: utf-8 -*-
"""Melissa_VIX_ID/X Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H1-Hjk_zsVEPbdh3l0WmQ-Oyxhg5__0y

#**Credit Loan Prediction**

#Business Problem

## Problem Statement

Risiko kredit adalah kerugian yang berhubungan dengan potensi kegagalan dalam memenuhi kewajiban membayar kredit ketika waktu jatuh tempo. Dengan kata lain, risiko kredit adalah kemungkinan debitur tidak mampu atau tidak ingin membayar pinjamannya pada saat jatuh tempo atau sesudahnya.

Salah satu faktor penyebab terjadinya risiko kredit adalah kesalahan penilaian dalam keputusan pemberian pinjaman oleh perusahaan kredit atau lembaga keuangan seperti bank. Jika perusahaan dapat menerapkan kebijakan pemberian kredit yang sehat, kemungkinan risiko masalah akan kecil.

## Analytic Approach

Membangun sebuah predictive model menggunakan algoritma machine learning yang akan dibuat berdasarkan loan dataset berisi transaksi yang pernah dilakukan oleh peminjam.

Model ini bertujuan untuk:
- mengoptimalkan akurasi prediksi risiko kredit untuk memperbaiki keputusan pemberian pinjaman kepada klien.
- mengurangi risiko kerugian tanpa adanya pembuatan keputusan transaksi peminjaman secara manual yang dapat menimbulkan human error.
- menciptakan dampak positif untuk perusahaan dengan pencegahan financial loss, serta memberi efisiensi dalam operasional perusahaan.

# Load Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
import matplotlib.font_manager as fm
from matplotlib.ticker import FuncFormatter
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

"""# Data Understanding

"""

#load data
df = pd.read_csv("loan_data_2007_2014.csv")

#value of (x, y) -> (rows, columns)
df.shape

#to view the first 5 rows
df.head()

#retrieve data column info
df.info()

"""# Data Cleansing

"""

#duplicated values
df.duplicated().sum()

#total missing values per column

dtypes = [df[col].dtype for col in df.columns]
items_count = len(df)
unique = (df[col].unique() for col in df.columns)
total_null = df.isnull().sum()
percentage_missing = round(df.isnull().sum() * 100 / df.shape[0])

df_missing = pd.DataFrame({'data_type': dtypes,
                                'count': items_count,
                                'unique': unique,
                                'total_missing': total_null,
                                'percent_missing': percentage_missing})
df_missing.sort_values('percent_missing', ascending = False,inplace = True)

df_missing

"""Ada beberapa fitur yang akan di drop yang berupa:
- Feature Unnamed: 0, id, and member_id yang memiliki jumlah nilai unique 466285, yang terlalu bervariasi, sehingga tidak terlalu relevan untuk melakukan prediksi loan credit.
- Kolom dengan jumlah missing value yang tinggi (>= 25%)
- Kolom yang termasuk target leakage
- Values yang kosong/null
"""

#dropping columns with a high amount of missing values
df.drop(columns=df_missing.loc[df_missing['percent_missing']> 25].index.tolist(), inplace = True)

#cleansing all NaN values
df = df.dropna(axis=1, how='all')

#number of columns now
df.shape[1]

#dropping irrelevant data
df.drop(columns=['id', 'member_id','url', 'title',
                 'addr_state','zip_code','policy_code',
                 'application_type','emp_title', 'sub_grade', 'Unnamed: 0',
                 'issue_d', 'earliest_cr_line', 'last_pymnt_d',
                 'last_credit_pull_d', 'pymnt_plan'], inplace = True)

#categorical statistic value
df.describe().T

#categorical statistic value
df.describe(include = 'O').T

"""# Exploratory Data Analysis

###Define Target
"""

#target feature: loan status
df['loan_status'].value_counts()

"""Kita akan membagi kolom loan_status menjadi 2 klasifikasi binary, dimana:

- 1 berarti peminjam kemungkinan akan membayar hutang dengan lancar  (suitable) yang berlaku untuk kolom-kolom Current, Fully Paid, dan In Grace Period
- 0 berarti peminjam kemungkinan tidak akan membayar hutang dengan lancar (not suitable)
"""

# listing values that are into suitable for loan approval list
suitable = ['Current', 'Fully Paid', 'In Grace Period']

# encoding loan_status
df['loan_status'] = np.where(df['loan_status'].isin(suitable), 1, 0)
df['loan_status'].value_counts()/len(df)*100

"""### Multivariate Analysis"""

plt.figure(figsize=(16,6))
mask = np.triu(np.ones_like(df.corr(), dtype=np.bool_))
heatmap = sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, cmap='GnBu',
          annot=True, fmt='.2f').set_title('Correlation Heatmap',
          fontdict={'fontsize':18}, pad=14)
plt.show()

"""Dari correlation heatmap diatas, kita akan drop feature dengan nilai ketergantungan lebih dari 0.7 untuk menghindari hasil-hasil yang kurang dapat diandalkan atau bersifat biased."""

#drop highly correlated features to prevent biased results
corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
high_correlation = [column for column in upper.columns if any(upper[column] > 0.7)]
high_correlation

df.drop(high_correlation, axis=1, inplace=True)

"""### Univariate Analysis"""

num = df.select_dtypes(include='number').columns
cat = df.select_dtypes(include='object').columns

#numerical features
plt.figure(figsize=(24,28))
for i in range(0,len(num)):
    plt.subplot(10,4,i+1)
    sns.kdeplot(x=df[num[i]], palette='rocket')
    plt.title(num[i], fontsize=20)
    plt.xlabel(' ')
    plt.tight_layout()

#categorical features
plt.figure(figsize=(20,20))
for i in range(0,len(cat)):
    plt.subplot(5,2,i+1)
    sns.countplot(y=df[cat[i]], orient = 'h',palette='magma')
    plt.title(cat[i])
    plt.xlabel(' ')
    plt.tight_layout()

"""### Bivariate Analysis"""

plt.figure(figsize=(20,20))
for i in range(0,len(cat)):
    plt.subplot(5,2,i+1)
    sns.countplot(y=df[cat[i]], palette='magma', hue=df['loan_status'])
    plt.title(cat[i])
    plt.xlabel(' ')
    plt.tight_layout()

"""## Feature Engineering with Weight of Evidence and Information Value"""

df_fe = df.copy()
# Create Function for Weight of Evidence and Invormation Value

def woe(df, feature_name):
    feature_name = df.groupby(feature_name).agg(num_observation=('loan_status','count'),good_loan_prob=('loan_status','mean')).reset_index()
    feature_name['grade_proportion'] = feature_name['num_observation']/feature_name['num_observation'].sum()
    feature_name['num_good_loan'] = feature_name['grade_proportion'] * feature_name['num_observation']
    feature_name['num_bad_loan'] = (1-feature_name['grade_proportion']) * feature_name['num_observation']
    feature_name['good_loan_prop'] = feature_name['num_good_loan'] / feature_name['num_good_loan'].sum()
    feature_name['bad_loan_prop'] = feature_name['num_bad_loan'] / feature_name['num_bad_loan'].sum()
    feature_name['weight_of_evidence'] = np.log(feature_name['good_loan_prop'] / feature_name['bad_loan_prop'])
    feature_name = feature_name.sort_values('weight_of_evidence').reset_index(drop=True)
    feature_name['information_value'] = (feature_name['good_loan_prop']-feature_name['bad_loan_prop']) * feature_name['weight_of_evidence']
    feature_name['information_value'] = feature_name['information_value'].sum()

    #Show
    feature_name = feature_name.drop(['grade_proportion','num_good_loan','num_bad_loan','good_loan_prop','bad_loan_prop'],axis = 1)
    return feature_name

"""###Categorical Features"""

#grade
woe(df_fe,'grade')

#emp_length
woe(df_fe,'emp_length')

#home_ownership
woe(df_fe,'home_ownership')

#verification_status
woe(df_fe,'verification_status')

#purpose
woe(df_fe,'purpose')

#initial_list_status
woe(df_fe,'initial_list_status')

#term
woe(df_fe,'term')

"""###Numerical Features"""

#loan_amnt
df_fe['loan_amnt_fe'] = pd.cut(df_fe['loan_amnt'],5)
woe(df_fe,'loan_amnt_fe')

#int_rate
df_fe['int_rate_fe'] = pd.cut(df_fe['int_rate'],5)
woe(df_fe,'int_rate_fe')

#annual_inc
df_fe['annual_inc_fe'] = np.where((df_fe['annual_inc']>=0)&(df_fe['annual_inc']<=200000),'low_income',
                               np.where((df_fe['annual_inc']>200000)&(df_fe['annual_inc']<=1500000),'med_income','high_income'))
woe(df_fe,'annual_inc_fe')

#dti
df_fe['dti_woe'] = pd.cut(df_fe['dti'],5)
woe(df_fe,'dti_woe')

#delinq_2yrs

# this feature will be encoded, if values = 0 return 0,
# if its greater than 0 return 1, if > 5, return 2
df_fe['delinq_2yrs_fs'] = np.where(df_fe['delinq_2yrs'] > 3, 3,
                                 np.where(df_fe['delinq_2yrs'] == 2, 2,
                                 np.where(df_fe['delinq_2yrs'] == 1,1,0)))
woe(df_fe,'delinq_2yrs_fs')

#inq_last_6mths
#encoding the feature
df_fe['inq_last_6mths_fe'] = np.where(df_fe['inq_last_6mths'] == 0,0,
                                    np.where((df_fe['inq_last_6mths'] > 0)&
                                    (df_fe['inq_last_6mths'] <=3),1,
                                    np.where((df_fe['inq_last_6mths']>3)&
                                    (df_fe['inq_last_6mths']<=6),2,
                                    np.where((df_fe['inq_last_6mths']>6)&
                                    (df_fe['inq_last_6mths']<=9),3,4))))
woe(df_fe,'inq_last_6mths_fe')

#open_acc
# refining class = 5 class
df_fe['open_acc_fe'] = pd.cut(df_fe['open_acc'], 5)
woe(df_fe,'open_acc_fe')

#pub_rec
# refining class = 5 class
df_fe['pub_rec_fe'] = pd.cut(df_fe['pub_rec'], 5)
woe(df_fe,'pub_rec_fe')

#revol_bal
#encoding the feature
df_fe['revol_bal_fe'] = np.where((df_fe['revol_bal']>=0)&(df_fe['revol_bal']<=5000),0,
                               np.where((df_fe['revol_bal']>5000)&(df_fe['revol_bal']<=10000),1,
                               np.where((df_fe['revol_bal']>10000)&(df_fe['revol_bal']<=15000),2,3)))
woe(df_fe,'revol_bal_fe')

#revol_util
#encoding the feature
df_fe['revol_util_fe'] = np.where((df_fe['revol_util']>=0)&(df_fe['revol_util']<=20),0,
                                  np.where((df_fe['revol_util']>20)&(df_fe['revol_util']<=40),1,
                                           np.where((df_fe['revol_util']>40)&(df_fe['revol_util']<=60),2,
                                                    np.where((df_fe['revol_util']>60)&(df_fe['revol_util']<=80),3,4))))
woe(df_fe,'revol_util_fe')

#total_acc
#encoding the feature
df_fe['total_acc_woe'] = pd.cut(df_fe['total_acc'],5)
woe(df_fe,'total_acc_woe')

#out_prncp
#Encoding into new class
df_fe['out_prncp_fe'] = np.where((df_fe['out_prncp']>=0)&(df_fe['out_prncp']<=1000),0,
                               np.where((df_fe['out_prncp']>1000)&(df_fe['out_prncp']<=10000),1,
                               np.where((df_fe['out_prncp']>10000)&(df_fe['out_prncp']<=17000),2,3)))
woe(df_fe,'out_prncp_fe')

#total_rec_late_fee
#Encoding into new class
df_fe['total_rec_late_fee_fe'] = np.where(df_fe['total_rec_late_fee']==0,0,1)
woe(df_fe,'total_rec_late_fee_fe')

#recoveries
df_fe['recoveries_fe'] = pd.cut(df_fe['recoveries'], 5)
woe(df_fe,'recoveries_fe')

#collections_12_mths_ex_med
df_fe['collections_12_mths_ex_med_fe'] = pd.cut(df_fe['collections_12_mths_ex_med'],5)
woe(df_fe,'collections_12_mths_ex_med_fe')

#acc_now_delinq
df_fe['acc_now_delinq_fe'] = pd.cut(df_fe['acc_now_delinq'],5)
woe(df_fe,'acc_now_delinq_fe')

#tot_coll_amt
df_fe['tot_coll_amt_fe'] = pd.cut(df_fe['tot_coll_amt'],5)
woe(df_fe,'tot_coll_amt_fe')

#tot_cur_bal
df_fe['tot_cur_bal_fe'] = pd.cut(df_fe['tot_cur_bal'],5)
woe(df_fe,'tot_cur_bal_fe')

"""Dari feature engineering diatas, kita akan menghapus fitur dengan information value < 0.02 (useless predictive) dan > 0.5 (suspicious predictive), serta fitur-fitur yang tidak relevan dan tidak masuk akal."""

drop_list = ['emp_length','verification_status', 'purpose', 'term','annual_inc',
             'delinq_2yrs', 'total_acc','open_acc', 'pub_rec', 'out_prncp',
             'total_rec_late_fee','recoveries', 'collections_12_mths_ex_med',
             'acc_now_delinq','tot_coll_amt','tot_cur_bal']

df.drop(drop_list, axis = 1, inplace=True)

"""##Feature Encoding"""

df_encode = df.copy()

"""###Label Encoding"""

print(df_encode['grade'].unique())
print(df_encode['home_ownership'].unique())
print(df_encode['initial_list_status'].unique())

df_encode['home_ownership'].value_counts()

# Replace label with same characteristic
target_dict = {'MORTGAGE':'MORTGAGE',
               'RENT':'RENT',
               'OWN':'OWN',
               'OTHER':'OTHER',
               'ANY':'OTHER',
               'NONE':'OTHER'}

df_encode["home_ownership"] = df_encode["home_ownership"].map(target_dict)

df_encode['initial_list_status'] = np.where(df_encode['initial_list_status']=='f',0,1)

"""###One Hot Encoding: Categorical"""

for cat in ['grade', 'home_ownership', 'initial_list_status']:
  onehots = pd.get_dummies(df[cat], prefix=cat)
  df_encode = df_encode.join(onehots)

df_encode = df_encode.drop(columns=['grade', 'home_ownership', 'initial_list_status'], axis =1)
df_encode.head()

df_encode.info()

"""###Standardization with Standard Scaler"""

from sklearn.preprocessing import StandardScaler

con_cols = [col for col in df_encode.columns.tolist()]
ss = StandardScaler()
std_cols = pd.DataFrame(ss.fit_transform(df_encode[con_cols]), columns=con_cols)

std_cols.info()

"""###One Hot Encoding: Numerical"""

from sklearn.preprocessing import LabelEncoder

#Label encoding

le = LabelEncoder()

columns = [ 'loan_amnt', 'int_rate', 'loan_status', 'dti', 'inq_last_6mths',
       'revol_bal', 'revol_util'
]

for col in columns:
    df_encode[col] = le.fit_transform(df[col])

df_encode.var()

df_encode.shape

df_encode.info()

"""#Modeling

##Split Train and Test Dataset
"""

from datetime import datetime as dt
from collections import defaultdict
from xgboost import XGBClassifier
import lightgbm as lgb
import time
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score, recall_score, precision_score,roc_auc_score, roc_curve, f1_score
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay

from sklearn.model_selection import train_test_split
# Pisahkan fitur (X) dan target (y)
X = df_encode.drop('loan_status', axis=1)
y = df_encode['loan_status']

# Bagi data menjadi set pelatihan dan set pengujian
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def modelling(x_train,x_test,y_train,y_test):
    result = defaultdict(list)

    lr = LogisticRegression()
    dt = DecisionTreeClassifier()
    ab = AdaBoostClassifier()
    nb = GaussianNB()
    knn = KNeighborsClassifier()
    xgb = XGBClassifier()
    rf = RandomForestClassifier()
    grad = GradientBoostingClassifier()
    LGBM = lgb.LGBMClassifier()


    list_model = [('Logistic Regression',lr),
                  ('Decision Tree',dt),
                  ('AdaBoost', ab),
                  ('Gaussian NB', nb),
                  ('K-Nearest Neighbor',knn),
                  ('XgBoost', xgb),
                  ('Random Forest',rf),
                  ('Gradient Boosting',grad),
                  ('LightGBM',LGBM)
                  ]

    for model_name, model in list_model:
        model.fit(x_train,y_train)
        y_pred_proba = model.predict_proba(x_test)

        y_pred = model.predict(x_test)

        accuracy = accuracy_score(y_test,y_pred)
        recall = recall_score(y_test,y_pred)
        precision = precision_score(y_test,y_pred)
        AUC = roc_auc_score(y_test, y_pred_proba[:, 1])

        result['model_name'].append(model_name)
        result['model'].append(model)
        result['accuracy'].append(accuracy)
        result['recall'].append(recall)
        result['precision'].append(precision)
        result['AUC'].append(AUC)

    return result

evaluation_summary = modelling(X_train, X_test, y_train,y_test)
evaluation_summary = pd.DataFrame(evaluation_summary)
evaluation_summary

"""Diantara semua model machine learning yang diuji, dengan mempertimbangkan nilai akurasi, precision, recall dan AUC paling tinggi, diputuskan untuk menggunakan metode Light GBM."""

lg_model = lgb.LGBMClassifier()
lg_model.fit(X_train, y_train)
y_pred_lg = lg_model.predict(X_test)

def show_cmatrix(ytest, pred):
    # Creating confusion matrix
    cm = confusion_matrix(ytest, pred)

    # Putting the matrix a dataframe form
    cm_df = pd.DataFrame(cm, index=['Bad Loan', 'Good Loan'],
                 columns=['Predicted Bad Loan', 'Predicted Good Loan'])

    # visualizing the confusion matrix
    sns.set(font_scale=1.2)
    plt.figure(figsize=(10,6))

    sns.heatmap(cm, annot=True, fmt='g', cmap="Reds",xticklabels=cm_df.columns, yticklabels=cm_df.index, annot_kws={"size": 20})
    plt.title("Confusion Matrix", size=20)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label');

show_cmatrix(y_test, y_pred_lg)

"""##Hyperparameter Tuning Menggunakan Metode Undersampling"""

from sklearn.model_selection import RandomizedSearchCV

def evaluation_tuned(model):
    y_pred = model.predict(X_test)
    y_pred_train = model.predict(X_train)
    y_pred_proba = model.predict_proba(X_test)
    y_pred_proba_train = model.predict_proba(X_train)

    print('**EVALUATION WITH UNDERSAMPLING**\nAUC Train vs Test:')
    print(f'AUC Score Train proba: {round(roc_auc_score(y_train,y_pred_proba_train[:,1]),6)}')
    print(f'AUC Score Test proba: {round(roc_auc_score(y_test,y_pred_proba[:,1]),6)}\n')

    print('Others Metrics Evaluation:')
    print(f'Test Accuracy Score : {round(accuracy_score(y_test,y_pred),4)}')
    print(f'Precision Score Test: {round(precision_score(y_test,y_pred),4)}')
    print(f'Recall Score Test : {round(recall_score(y_test,y_pred),4)}')
    print(f'F1 Score Test : {round(f1_score(y_test,y_pred),4)}\n')

    print('**CONFUSION MATRIX**')
    print(confusion_matrix(y_test, y_pred))

lgbm = lgb.LGBMClassifier()
lgbm_hyparameters = {'max_depth': [4, 5], #
                     'min_child_samples': [20, 25], #
                     'learning_rate': [0.8, 0.9], #
                     'num_leaves': [15, 17], #
                     'subsample': [0.3, 0.4], #
                     'colsample_bytree': [0.8, 0.85],
                     'reg_alpha': [0.15, 0.19], #
                     'reg_lambda': [0.1, 0.8]} #

tuned_lgbm = RandomizedSearchCV(lgbm, lgbm_hyparameters, cv=5, random_state=69, n_jobs=-1, scoring='recall')
tuned_lgbm.fit(X_train, y_train)

tuned_lgbm.best_params_

"""###Evaluation After Hyperparameter Tuning"""

evaluation_tuned(tuned_lgbm)

"""Setelah melakukan hyperparameter tuning, terdapat sedikit penurunan performa pada model, namun nilai metrik-metrik tetap termasuk tinggi.

Hasil tetap menunjukkan bahwa model masih memiliki kemampuan yang baik dalam membedakan antara kelas positif dan negatif pada dataset.

# Feature Importance
"""

feat_importances = pd.Series(lg_model.feature_importances_, index=X.columns)
ax = feat_importances.nlargest(10).plot(kind='barh', figsize=(10, 8),color='maroon')
ax.invert_yaxis()

"""##Conclusion

Setelah dilakukan tuning pada model Light GBM pada dataset credit risk, int_rate (tingkat bunga) memiliki tingkat kepentingan paling tinggi.

Dari 10 fitur tersebut, 2 di antaranya berkaitan dengan jumlah pinjaman dan jumlah utang nasabah (loan_amnt dan revol_bal), 2 berkaitan dengan rasio utang dan pendapatan (dti dan annual_inc), 1 berkaitan dengan tingkat bunga (int_rate) dan 1 berkaitan dengan rasio penggunaan kredit (revol_util). Dari atas juga bisa disimpulkan jika nasabah dengan status pemilikan rumah RENT (sewa) atau MORTGAGE (pergadaian) cenderung lebih banyak membayar utang tepat waktu.

Hal ini menunjukkan bahwa faktor-faktor yang berhubungan dengan jumlah dan rasio hutang, pendapatan, tingkat bunga, dan penggunaan kredit merupakan faktor penting yang perlu diperhatikan dalam menganalisis risiko kredit nasabah.
"""